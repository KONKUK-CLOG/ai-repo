"""LLM agent endpoints for natural language command execution."""
from fastapi import APIRouter, Depends, HTTPException, status
from src.server.deps import get_current_user
from src.models.user import User
from src.server.schemas import (
    LLMExecuteRequest,
    LLMExecuteResult,
    ToolCall
)
from src.server.settings import settings
from openai import AsyncOpenAI
from src.mcp.tools import (
    post_blog_article,
    update_code_index,
    publish_to_notion,
    create_commit_and_push
)
import logging
import json

router = APIRouter(prefix="/api/v1/llm", tags=["llm-agent"])
logger = logging.getLogger(__name__)

# ============================================================================
# íˆ´ ë ˆì§€ìŠ¤íŠ¸ë¦¬ (Tool Registry)
# ============================================================================

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  íˆ´ì˜ ì¤‘ì•™ ë ˆì§€ìŠ¤íŠ¸ë¦¬
# agent.pyì™€ commands.pyì—ì„œ ê³µìœ í•˜ì—¬ ì‚¬ìš©
TOOLS_REGISTRY = {
    "post_blog_article": post_blog_article,           # ë¸”ë¡œê·¸ ê¸€ ë°œí–‰
    "update_code_index": update_code_index,           # ì½”ë“œ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
    "publish_to_notion": publish_to_notion,           # Notion í˜ì´ì§€ ë°œí–‰
    "create_commit_and_push": create_commit_and_push, # Git ì»¤ë°‹ & í‘¸ì‹œ
}


async def _execute_regular_tool(tool_name: str, params: dict) -> dict:
    """Execute a regular tool by name with given parameters.
    
    Args:
        tool_name: Name of the tool to execute
        params: Parameters for the tool
        
    Returns:
        Tool execution result
        
    Raises:
        Exception: If tool not found or execution fails
    """
    if tool_name not in TOOLS_REGISTRY:
        raise ValueError(f"Tool '{tool_name}' not found")
    
    tool_module = TOOLS_REGISTRY[tool_name]
    if not hasattr(tool_module, "run"):
        raise ValueError(f"Tool '{tool_name}' has no run method")
    
    return await tool_module.run(params)


def _calculate_dynamic_top_k(max_tokens: int) -> int:
    """Calculate optimal top_k based on LLM max_tokens setting.
    
    Args:
        max_tokens: Maximum tokens for LLM
        
    Returns:
        Optimal number of documents to retrieve
    """
    # Rough estimation: 
    # - Each code file ~500 tokens on average
    # - Reserve 50% of context for prompt + response
    available_tokens = max_tokens * 0.5
    top_k = int(available_tokens / 500)
    
    # Clamp between 3 and 30
    return max(3, min(30, top_k))


async def _execute_blog_article_with_rag(
    prompt: str,
    params: dict,
    user: User,
    model: str = None
) -> dict:
    """Execute blog article posting with RAG-enhanced content generation.
    
    ì´ í•¨ìˆ˜ëŠ” 2ë‹¨ê³„ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    1. RAGë¡œ ê´€ë ¨ ì½”ë“œë² ì´ìŠ¤ ê²€ìƒ‰
    2. LLMì´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¸”ë¡œê·¸ ê¸€ ìƒì„±
    
    Args:
        prompt: User's original prompt
        params: Tool parameters (may contain partial info)
        user: Authenticated user
        model: LLM model to use
        
    Returns:
        Blog article publication result
    """
    from src.adapters import vector_db
    
    logger.info(f"Executing blog article with RAG for prompt: {prompt}")
    
    # 1. Calculate optimal top_k and split between DBs
    max_tokens = settings.LLM_MAX_TOKENS
    total_top_k = _calculate_dynamic_top_k(max_tokens)
    
    # Split top_k: 70% for Vector DB (code content), 30% for Graph DB (structure)
    vector_top_k = max(3, int(total_top_k * 0.7))  # At least 3
    graph_top_k = max(2, int(total_top_k * 0.3))   # At least 2
    
    logger.info(f"Allocating top_k - Total: {total_top_k}, Vector: {vector_top_k}, Graph: {graph_top_k}")
    
    # 2. Perform RAG search using user's prompt
    # 2-1. Vector DB: Semantic search (priority - full code content)
    vector_results = await vector_db.semantic_search(
        collection=settings.VECTOR_DB_COLLECTION,
        query=prompt,
        user_id=user.id,
        top_k=vector_top_k
    )
    logger.info(f"Vector DB search returned {len(vector_results)} documents")
    
    # 2-2. Track files from Vector DB to avoid duplicates
    vector_files = {result['file'] for result in vector_results}
    
    # 2-3. Graph DB: Related code entities search
    from src.adapters import graph_db
    graph_results_raw = await graph_db.search_related_code(
        query=prompt,
        user_id=user.id,
        limit=graph_top_k * 3  # Get more to compensate for filtering
    )
    
    # 2-4. Filter out files already in Vector DB results (deduplication)
    graph_results = [
        r for r in graph_results_raw 
        if r['file'] not in vector_files
    ][:graph_top_k]  # Limit to graph_top_k
    
    logger.info(f"Graph DB search returned {len(graph_results_raw)} entities, "
                f"{len(graph_results)} unique after deduplication")
    
    # 3. Format RAG results for LLM context
    rag_context = []
    
    # 3-1. Vector DB results: Full code content (í•µì‹¬ ì½”ë“œ)
    if vector_results:
        rag_context.append("## ğŸ“„ í•µì‹¬ ì½”ë“œ (ì˜ë¯¸ì  ìœ ì‚¬ë„)\n")
        for idx, result in enumerate(vector_results, 1):
            rag_context.append(
                f"**{idx}. {result['file']}** (ìœ ì‚¬ë„: {result['score']:.3f})\n"
                f"```\n{result['content']}\n```\n"
            )
    
    # 3-2. Graph DB results: Concise entity descriptions (ì¶”ê°€ ê´€ë ¨ ì—”í‹°í‹°)
    if graph_results:
        rag_context.append("\n## ğŸ”— ì¶”ê°€ ê´€ë ¨ ì½”ë“œ ì—”í‹°í‹°\n")
        for result in graph_results:
            calls_info = ""
            if result.get("calls"):
                calls_list = ', '.join(result['calls'][:3])
                calls_info = f" (calls: {calls_list})"
            
            # Single-line format for efficiency
            rag_context.append(
                f"- **{result['file']}**: `{result['entity_name']}` "
                f"({result['entity_type']}){calls_info}\n"
            )
    
    rag_context_str = "\n".join(rag_context) if rag_context else "ê´€ë ¨ ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # 4. Call LLM to generate blog content
    if not settings.OPENAI_API_KEY:
        # Fallback: use simple content generation
        return await post_blog_article.run({
            "title": params.get("title", "ìë™ ìƒì„±ëœ ê¸€"),
            "markdown": params.get("markdown", f"# ì½”ë“œ ë³€ê²½ ìš”ì•½\n\n{prompt}"),
            "tags": params.get("tags", [])
        })
    
    try:
        client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        
        system_prompt = """ë‹¹ì‹ ì€ ê¸°ìˆ  ë¸”ë¡œê·¸ ì‘ì„±ìì…ë‹ˆë‹¤. 
ì œê³µëœ ì½”ë“œë² ì´ìŠ¤ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìµí•œ ê¸°ìˆ  ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
- ì œëª©ê³¼ ë³¸ë¬¸ì„ markdown í˜•ì‹ìœ¼ë¡œ ì‘ì„±
- ì½”ë“œ ì˜ˆì œë¥¼ ì ì ˆíˆ í™œìš©
- ê¸°ìˆ ì  ì •í™•ì„± ìœ ì§€
- ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…
- JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ: {"title": "...", "markdown": "..."}"""

        user_message = f"""ì‚¬ìš©ì ìš”ì²­: {prompt}

ê´€ë ¨ ì½”ë“œë² ì´ìŠ¤:
{rag_context_str}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. 
ì œëª©ê³¼ ë§ˆí¬ë‹¤ìš´ ë³¸ë¬¸ì„ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”."""

        response = await client.chat.completions.create(
            model=model or settings.DEFAULT_LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            temperature=0.7,
            max_tokens=max_tokens,
            response_format={"type": "json_object"}
        )
        
        content = response.choices[0].message.content
        blog_data = json.loads(content)
        
        # 5. Publish blog article with generated content
        result = await post_blog_article.run({
            "title": blog_data.get("title", params.get("title", "ìë™ ìƒì„±ëœ ê¸€")),
            "markdown": blog_data.get("markdown", params.get("markdown", "")),
            "tags": params.get("tags", [])
        })
        
        logger.info(f"Successfully published blog article with RAG")
        return result
        
    except Exception as e:
        logger.error(f"Failed to generate blog content with LLM: {e}")
        # Fallback to simple generation
        return await post_blog_article.run({
            "title": params.get("title", "ìë™ ìƒì„±ëœ ê¸€"),
            "markdown": params.get("markdown", f"# {prompt}\n\nê´€ë ¨ ì½”ë“œ:\n{rag_context_str}"),
            "tags": params.get("tags", [])
        })


async def call_llm_with_tools(
    prompt: str,
    context: dict,
    available_tools: list,
    model: str = None
) -> tuple[str, list[dict]]:
    """Call OpenAI GPT with available tools and get tool calls.
    
    OpenAI GPT APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©ìì˜ ìì—°ì–´ ëª…ë ¹ì„ ë¶„ì„í•˜ê³ 
    ì ì ˆí•œ íˆ´ì„ ì„ íƒí•©ë‹ˆë‹¤.
    
    Args:
        prompt: User's natural language command
        context: Additional context
        available_tools: List of available tool schemas
        model: LLM model to use
        
    Returns:
        Tuple of (thought_process, tool_calls_to_make)
    """
    logger.info(f"LLM called with prompt: {prompt}")
    logger.info(f"Available tools: {[t['name'] for t in available_tools]}")
    logger.info(f"Context keys: {list(context.keys())}")
    
    # API í‚¤ í™•ì¸
    if not settings.OPENAI_API_KEY:
        logger.warning("OPENAI_API_KEY not set - using fallback dummy logic")
        return _fallback_tool_selection(prompt, context)
    
    # 1. OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
    
    # 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """ë‹¹ì‹ ì€ ì½”ë“œ ê´€ë¦¬ ë° ë¬¸ì„œí™” ì‘ì—…ì„ ë•ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ íˆ´ì„ ì„ íƒí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ íˆ´:
- post_blog_article: ë¸”ë¡œê·¸ì— ê¸€ ë°œí–‰
- update_code_index: ì½”ë“œ ë³€ê²½ì‚¬í•­ì„ ë²¡í„°/ê·¸ë˜í”„ ì¸ë±ìŠ¤ì— ë°˜ì˜
- publish_to_notion: Notionì— í˜ì´ì§€ ë°œí–‰
- create_commit_and_push: Git ì»¤ë°‹ í›„ í‘¸ì‹œ

ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ì ì ˆí•œ íŒŒë¼ë¯¸í„°ë¥¼ êµ¬ì„±í•˜ì„¸ìš”."""
    
    # 3. ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„±
    context_str = json.dumps(context, ensure_ascii=False, indent=2) if context else "ì—†ìŒ"
    user_message = f"""ì‚¬ìš©ì ìš”ì²­: {prompt}

ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸:
{context_str}

ìœ„ ìš”ì²­ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í•„ìš”í•œ íˆ´ì„ ì„ íƒí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”."""
    
    # 4. íˆ´ ìŠ¤í‚¤ë§ˆë¥¼ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    openai_tools = []
    for tool in available_tools:
        openai_tools.append({
            "type": "function",
            "function": {
                "name": tool["name"],
                "description": tool.get("description", ""),
                "parameters": tool.get("input_schema", {})
            }
        })
    
    # 5. LLM í˜¸ì¶œ
    try:
        response = await client.chat.completions.create(
            model=model or settings.DEFAULT_LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            tools=openai_tools,
            tool_choice="auto",
            temperature=settings.LLM_TEMPERATURE,
            max_tokens=settings.LLM_MAX_TOKENS
        )
        
        logger.info(f"LLM response received from {response.model}")
        
        # 6. ì‘ë‹µ íŒŒì‹±
        message = response.choices[0].message
        thought = message.content or "íˆ´ ì‹¤í–‰ì„ ì‹œì‘í•©ë‹ˆë‹¤."
        tool_calls = []
        
        if message.tool_calls:
            for tool_call in message.tool_calls:
                try:
                    params = json.loads(tool_call.function.arguments)
                    tool_calls.append({
                        "tool": tool_call.function.name,
                        "params": params
                    })
                    logger.info(f"Tool selected: {tool_call.function.name}")
                except json.JSONDecodeError as e:
                    logger.error(f"Failed to parse tool arguments: {e}")
                    continue
        
        # íˆ´ì´ ì„ íƒë˜ì§€ ì•Šì€ ê²½ìš°
        if not tool_calls:
            logger.warning("LLM did not select any tools")
            thought = thought or "ìš”ì²­ì„ ì²˜ë¦¬í•  ì ì ˆí•œ íˆ´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        return thought, tool_calls
        
    except Exception as e:
        logger.error(f"LLM API call failed: {e}")
        # í´ë°±: ë”ë¯¸ ë¡œì§ ì‚¬ìš©
        return _fallback_tool_selection(prompt, context)


def _fallback_tool_selection(prompt: str, context: dict) -> tuple[str, list[dict]]:
    """Fallback tool selection when LLM API is unavailable.
    
    API í‚¤ê°€ ì—†ê±°ë‚˜ LLM í˜¸ì¶œì´ ì‹¤íŒ¨í•œ ê²½ìš° ì‚¬ìš©ë˜ëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ë¡œì§.
    """
    logger.info("Using fallback keyword-based tool selection")
    thought = "LLM APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
    tool_calls = []
    
    prompt_lower = prompt.lower()
    
    # í‚¤ì›Œë“œ ê¸°ë°˜ ë”ë¯¸ ë¡œì§
    if "ì¸ë±ìŠ¤" in prompt_lower or "index" in prompt_lower:
        if "diff" in context or "files" in context:
            tool_calls.append({
                "tool": "update_code_index",
                "params": {
                    "files": context.get("diff", {}).get("files", [])
                }
            })
    
    if "ë¸”ë¡œê·¸" in prompt_lower or "blog" in prompt_lower or "ê¸€" in prompt_lower:
        tool_calls.append({
            "tool": "post_blog_article",
            "params": {
                "title": "ìë™ ìƒì„±ëœ ê¸€",
                "markdown": f"# ì½”ë“œ ë³€ê²½ ìš”ì•½\n\n{prompt}"
            }
        })
    
    if "ë…¸ì…˜" in prompt_lower or "notion" in prompt_lower:
        tool_calls.append({
            "tool": "publish_to_notion",
            "params": {
                "title": "ìë™ ìƒì„± í˜ì´ì§€",
                "content": prompt
            }
        })
    
    if "ì»¤ë°‹" in prompt_lower or "commit" in prompt_lower or "push" in prompt_lower:
        tool_calls.append({
            "tool": "create_commit_and_push",
            "params": {
                "repo_path": context.get("repo_path", "."),
                "files": context.get("files", []),
                "commit_message": "Auto commit"
            }
        })
    
    # ì•„ë¬´ íˆ´ë„ ì„ íƒë˜ì§€ ì•Šì€ ê²½ìš°
    if not tool_calls:
        thought = "ìš”ì²­ì„ ì²˜ë¦¬í•  ì ì ˆí•œ íˆ´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì‘ì—…ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”."
    
    return thought, tool_calls


@router.post("/execute", response_model=LLMExecuteResult)
async def execute_llm_command(
    request: LLMExecuteRequest,
    user: User = Depends(get_current_user)
) -> LLMExecuteResult:
    """ì‚¬ìš©ìì˜ ìì—°ì–´ ëª…ë ¹ì„ LLMì´ í•´ì„í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ë‹¤ìŒ ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤:
    1. ì‚¬ìš©ìì˜ ìì—°ì–´ ëª…ë ¹ì„ ë°›ìŒ
    2. LLMì—ê²Œ ëª…ë ¹ê³¼ ì‚¬ìš© ê°€ëŠ¥í•œ íˆ´ ëª©ë¡ì„ ì „ë‹¬
    3. LLMì´ ì–´ë–¤ íˆ´ì„ ì–´ë–¤ ìˆœì„œë¡œ ì‹¤í–‰í• ì§€ ê²°ì •
    4. ì„ íƒëœ íˆ´ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
    5. ê° íˆ´ì˜ ê²°ê³¼ë¥¼ LLMì—ê²Œ í”¼ë“œë°±
    6. LLMì˜ ìµœì¢… ì‘ë‹µì„ ì‚¬ìš©ìì—ê²Œ ë°˜í™˜
    
    Args:
        request: LLM ì‹¤í–‰ ìš”ì²­ (í”„ë¡¬í”„íŠ¸, ì»¨í…ìŠ¤íŠ¸ ë“±)
        api_key: ì¸ì¦ëœ API í‚¤
        
    Returns:
        LLM ì‹¤í–‰ ê²°ê³¼ (ì‚¬ê³  ê³¼ì •, íˆ´ í˜¸ì¶œ ëª©ë¡, ìµœì¢… ì‘ë‹µ)
        
    Raises:
        HTTPException: 400 if invalid request, 500 if execution fails
    """
    logger.info(f"LLM execute request: {request.prompt}")
    
    try:
        # 1. ì‚¬ìš© ê°€ëŠ¥í•œ íˆ´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        available_tools = []
        for tool_name, tool_module in TOOLS_REGISTRY.items():
            if hasattr(tool_module, "TOOL"):
                available_tools.append(tool_module.TOOL)
        
        # 2. LLM í˜¸ì¶œí•˜ì—¬ ì‹¤í–‰í•  íˆ´ ê²°ì •
        thought, tool_calls_to_make = await call_llm_with_tools(
            prompt=request.prompt,
            context=request.context,
            available_tools=available_tools,
            model=request.model
        )
        
        # 3. ì„ íƒëœ íˆ´ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
        executed_tool_calls = []
        for tool_call_plan in tool_calls_to_make:
            tool_name = tool_call_plan["tool"]
            params = tool_call_plan["params"]
            
            try:
                # ë¸”ë¡œê·¸ ê¸€ ì‘ì„±ì€ RAGë¥¼ ì‚¬ìš©í•œ 2ë‹¨ê³„ ì¶”ë¡ 
                if tool_name == "post_blog_article":
                    logger.info("Using RAG-enhanced execution for blog article")
                    result = await _execute_blog_article_with_rag(
                        prompt=request.prompt,
                        params=params,
                        user=user,
                        model=request.model
                    )
                else:
                    # ë‹¤ë¥¸ íˆ´ë“¤ì€ ì¼ë°˜ ì‹¤í–‰
                    result = await _execute_regular_tool(tool_name, params)
                
                executed_tool_calls.append(ToolCall(
                    tool=tool_name,
                    params=params,
                    result=result,
                    success=True
                ))
                logger.info(f"Successfully executed tool: {tool_name}")
            except Exception as e:
                logger.error(f"Failed to execute tool {tool_name}: {e}")
                executed_tool_calls.append(ToolCall(
                    tool=tool_name,
                    params=params,
                    result={"error": str(e)},
                    success=False
                ))
        
        # 4. ìµœì¢… ì‘ë‹µ ìƒì„±
        successful_tools = [tc.tool for tc in executed_tool_calls if tc.success]
        failed_tools = [tc.tool for tc in executed_tool_calls if not tc.success]
        
        # LLMì—ê²Œ íˆ´ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì „ë‹¬í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±
        if settings.OPENAI_API_KEY:
            try:
                final_response = await _generate_final_response(
                    request.prompt,
                    executed_tool_calls,
                    request.model
                )
            except Exception as e:
                logger.error(f"Failed to generate final response from LLM: {e}")
                # í´ë°± ì‘ë‹µ
                final_response = _create_fallback_response(successful_tools, failed_tools)
        else:
            final_response = _create_fallback_response(successful_tools, failed_tools)
        
        return LLMExecuteResult(
            ok=len(failed_tools) == 0,
            thought=thought,
            tool_calls=executed_tool_calls,
            final_response=final_response,
            model_used=request.model or settings.DEFAULT_LLM_MODEL
        )
        
    except Exception as e:
        logger.error(f"Error executing LLM command: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to execute LLM command: {str(e)}"
        )


async def _generate_final_response(
    original_prompt: str,
    tool_calls: list[ToolCall],
    model: str = None
) -> str:
    """Generate final user-friendly response using LLM.
    
    íˆ´ ì‹¤í–‰ ê²°ê³¼ë¥¼ LLMì—ê²Œ ì „ë‹¬í•˜ì—¬ ì‚¬ìš©ì ì¹œí™”ì ì¸ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
    
    # íˆ´ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½
    tool_results_summary = []
    for tc in tool_calls:
        status = "âœ… ì„±ê³µ" if tc.success else "âŒ ì‹¤íŒ¨"
        tool_results_summary.append(
            f"{status} {tc.tool}: {json.dumps(tc.result, ensure_ascii=False)[:200]}"
        )
    
    summary_text = "\n".join(tool_results_summary)
    
    response = await client.chat.completions.create(
        model=model or settings.DEFAULT_LLM_MODEL,
        messages=[
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ ì‘ì—… ê²°ê³¼ë¥¼ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ì „ë‹¬í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
            },
            {
                "role": "user",
                "content": f"""ì‚¬ìš©ì ìš”ì²­: {original_prompt}

ì‹¤í–‰ëœ ì‘ì—… ê²°ê³¼:
{summary_text}

ìœ„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê³  ëª…í™•í•œ ìµœì¢… ì‘ë‹µì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ê°„ê²°í•˜ê²Œ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”."""
            }
        ],
        temperature=0.7,
        max_tokens=500
    )
    
    return response.choices[0].message.content or "ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."


def _create_fallback_response(successful_tools: list[str], failed_tools: list[str]) -> str:
    """Create fallback response when LLM is unavailable."""
    if failed_tools:
        return (
            f"ì¼ë¶€ ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. "
            f"ì„±ê³µ: {', '.join(successful_tools) if successful_tools else 'ì—†ìŒ'}, "
            f"ì‹¤íŒ¨: {', '.join(failed_tools)}"
        )
    else:
        return (
            f"ìš”ì²­í•˜ì‹  ì‘ì—…ì„ ëª¨ë‘ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. "
            f"ì‹¤í–‰ëœ ì‘ì—…: {', '.join(successful_tools)}"
        )

